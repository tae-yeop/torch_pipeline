{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 forward를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def torch_dfs(model):\n",
    "    result = [model]\n",
    "    # modle의 children method를 활용\n",
    "    for child in model.children():\n",
    "        # recursion\n",
    "        result += torch_dfs(child)\n",
    "    # list를 리턴해서 append되도록 함\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 새로운 forward를 설정하도록 함\n",
    "        if reference_adain:\n",
    "            attn_modules = [module for module in torch_dfs(self.unet) if isinstance(module, BasicTransformerBlock)]\n",
    "            # 깊어질수록 LayerNorm의 channel 값이 커진다\n",
    "            # 낮은 채널 값으로 sorting하도록 함\n",
    "            attn_modules = sorted(attn_modules, key=lambda x : -x.norm1.normalized_shape[0])\n",
    "\n",
    "            for i, module in enumerate(attn_modules):\n",
    "                module._original_inner_forward = module.forward\n",
    "                module.forward = hacked_basic_transformer_inner_forward.__get__(module, BasicTransformerBlock)\n",
    "                module.bank = []\n",
    "                module.attn_weight = float(i) / float(len(attn_modules))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
