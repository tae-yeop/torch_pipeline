{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import inspect\n",
    "\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    adam8bit_class = bnb.optim.Adam8bit\n",
    "except ImportError:\n",
    "    adam8bit_class = None\n",
    "    # pass, raise ImportError\n",
    "\n",
    "try:\n",
    "    import prodigyopt\n",
    "    prodigy_class = prodigyopt.Prodigy\n",
    "except ImportError:\n",
    "    prodigy_class = None\n",
    "\n",
    "optimizer_dict = {'adam': torch.optim.Adam, 'adam8bit': adam8bit_class, 'adamw': torch.optim.AdamW, 'prodigy': prodigy_class}\n",
    "\n",
    "def filter_valid_params(constructor, params_dict):\n",
    "    valid_params = inspect.signature(constructor).parameters\n",
    "    filtered_params = {key: value for key, value in params_dict.items() if key in valid_params}\n",
    "    return filtered_params\n",
    "\n",
    "def prepare_optimizer_params(models, learning_rates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model1 = torch.nn.Linear(3,4)\n",
    "model2 = torch.nn.Conv2d(3, 6)\n",
    "# 메모리 줄이기\n",
    "model1_parameters = list(filter(lambda p: p.requires_grad, model1.parameters()))\n",
    "model1_parameters_with_lr = {\"params\": model1_parameters, \"lr\": 0.15}\n",
    "\n",
    "model2_parameters = list(filter(lambda p: p.requires_grad, model2.parameters()))\n",
    "model2_parameters_with_lr = {\"params\": model2_parameters, \"lr\": 0.1}\n",
    "\n",
    "\n",
    "params_to_optimizer = [model1_parameters_with_lr, model2_parameters_with_lr]\n",
    "my_params = {'betas' : (0.1, 0.1), 'weight_decay' : 0.99, 'eps':0.999, 'lr': 0.01}\n",
    "for key, optimizer_class in optimizer_dict.items():\n",
    "    if optimizer_class is None:\n",
    "        continue\n",
    "    if key == 'adam8bit':\n",
    "        \n",
    "        my_params['lr'] = 0.15\n",
    "    valid_params = filter_valid_params(optimizer_class, my_params)\n",
    "    print(valid_params)\n",
    "    optimizer = optimizer_class(params_to_optimize, **valid_params)\n",
    "    print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Callable\n",
    "\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# update functions\n",
    "\n",
    "def update_fn(p, grad, exp_avg, lr, wd, beta1, beta2):\n",
    "    # stepweight decay\n",
    "\n",
    "    p.data.mul_(1 - lr * wd)\n",
    "\n",
    "    # weight update\n",
    "\n",
    "    update = exp_avg.clone().mul_(beta1).add(grad, alpha = 1 - beta1).sign_()\n",
    "    p.add_(update, alpha = -lr)\n",
    "\n",
    "    # decay the momentum running average coefficient\n",
    "\n",
    "    exp_avg.mul_(beta2).add_(grad, alpha = 1 - beta2)\n",
    "\n",
    "class Lion(Optimizer):\n",
    "    # https://github.com/lucidrains/lion-pytorch/blob/main/lion_pytorch/lion_pytorch.py\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-4,\n",
    "        betas: Tuple[float, float] = (0.9, 0.99),\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        assert lr > 0.\n",
    "        assert all([0. <= beta <= 1. for beta in betas])\n",
    "\n",
    "        defaults = dict(\n",
    "            lr = lr,\n",
    "            betas = betas,\n",
    "            weight_decay = weight_decay\n",
    "        )\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        self.update_fn = update_fn\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(\n",
    "        self,\n",
    "        closure: Optional[Callable] = None\n",
    "    ):\n",
    "\n",
    "        loss = None\n",
    "        if exists(closure):\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in filter(lambda p: exists(p.grad), group['params']):\n",
    "\n",
    "                grad, lr, wd, beta1, beta2, state = p.grad, group['lr'], group['weight_decay'], *group['betas'], self.state[p]\n",
    "\n",
    "                # init state - exponential moving average of gradient values\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "\n",
    "                exp_avg = state['exp_avg']\n",
    "\n",
    "                self.update_fn(\n",
    "                    p,\n",
    "                    grad,\n",
    "                    exp_avg,\n",
    "                    lr,\n",
    "                    wd,\n",
    "                    beta1,\n",
    "                    beta2\n",
    "                )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.init import orthogonal_\n",
    "import math\n",
    "\n",
    "\n",
    "def singular_value(p):\n",
    "    sv = math.sqrt(p.shape[0] / p.shape[1])\n",
    "    if p.dim() == 4:\n",
    "        sv /= math.sqrt(p.shape[2] * p.shape[3])\n",
    "    return sv\n",
    "\n",
    "class AGD:\n",
    "    # https://github.com/jxbz/agd\n",
    "    @torch.no_grad()\n",
    "    def __init__(self, net, gain=1.0):\n",
    "\n",
    "        self.net = net\n",
    "        self.depth = len(list(net.parameters()))\n",
    "        self.gain = gain\n",
    "\n",
    "        for p in net.parameters():\n",
    "            if p.dim() == 1: raise Exception(\"Biases are not supported.\")\n",
    "            if p.dim() == 2: orthogonal_(p)\n",
    "            if p.dim() == 4:\n",
    "                for kx in range(p.shape[2]):\n",
    "                    for ky in range(p.shape[3]):\n",
    "                        orthogonal_(p[:,:,kx,ky])\n",
    "            p *= singular_value(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "\n",
    "        G = 0\n",
    "        for p in self.net.parameters():\n",
    "            G += singular_value(p) * p.grad.norm(dim=(0,1)).sum()\n",
    "        G /= self.depth\n",
    "\n",
    "        log = math.log(0.5 * (1 + math.sqrt(1 + 4*G)))\n",
    "\n",
    "        for p in self.net.parameters():\n",
    "            factor = singular_value(p) / p.grad.norm(dim=(0,1), keepdim=True)\n",
    "            p -= self.gain * log / self.depth * factor * p.grad\n",
    "\n",
    "        return log"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
