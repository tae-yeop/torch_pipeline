{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.utils import BaseOutput\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "\n",
    "\n",
    "\n",
    "from .embeddings import GaussianFourierProjection, TimestepEmbedding, Timesteps\n",
    "from .modeling_utils import ModelMixin\n",
    "from .unet_2d_blocks import UNetMidBlock2D, get_down_block, get_up_block\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UNet2DOutput(BaseOutput):\n",
    "    sample: torch.FloatTensor\n",
    "\n",
    "class UNet2DModel(ModelMixin, ConfigMixin):\n",
    "    @register_to_config\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, sample,\n",
    "                timestep: Union[torch.Tensor, float, int],\n",
    "                encoder_hidden_states,\n",
    "                down_block_additional_residuals = None, # adapter 혹은 controlnet에서 오는 feature\n",
    "                mid_block_additional_residual = None, # adapter 혹은 controlnet에서 오는 feature\n",
    "                **kwargs\n",
    "                ):\n",
    "\n",
    "        ...\n",
    "        # 0. 설정\n",
    "        # up block에서 upsample 해서 해상도를 키워야 하는 단계가 있다\n",
    "        forward_upsample_size = False\n",
    "\n",
    "        # 만약에 UNet으로 들어가는 인풋 (b, 4, 64, 64)의 해상도 64, 64만큼 나중에 upblock에서 만들어내야한다\n",
    "        # 그런데 현재 UNet의 upblock에 있는 upsampler 갯수로 불충분한 경우 64, 64로 다시 못 키우는 경우를 대비해서\n",
    "        if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n",
    "            logger.info(\"Forward upsample size to force interpolation output size.\")\n",
    "            forward_upsample_size = True\n",
    "\n",
    "        \n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        ...\n",
    "        # time embedding을 얻음\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "        emb = self.time_embedding(t_emb, timestep_cond)\n",
    "\n",
    "\n",
    "        # 2. pre-process\n",
    "        sample = self.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        # controlnet 혹은 adapter 쪽에서 오는 인풋이 있는지 체크\n",
    "        # controlnet의 mid block과 down block 쪽에서 나온 feature가 있는지 체크\n",
    "        is_controlnet = mid_block_additional_residual is not None and down_block_additional_residuals is not None\n",
    "        # adapter의 mid block은 없으나 down block 쪽에서 나오는 feature가 있는지 체크\n",
    "        is_adapter = mid_block_additional_residual is None and down_block_additional_residuals is not None\n",
    "\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            # 보통 CA를 있기 때문에 여길 실행\n",
    "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                # 만약 T2I adapter를 사용하는 상황이면 down_block_additional_residuals에 있는 첫번째 값을 down block에 전달\n",
    "                additional_residuals = {}\n",
    "                if is_adapter and len(down_block_additional_residuals) > 0:\n",
    "                    additional_residuals[\"additional_residuals\"] = down_block_additional_residuals.pop(0)\n",
    "                    \n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    **additional_residuals,\n",
    "                )\n",
    "            else:\n",
    "                ...\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "        # 만약 controlnet을 쓰는거라면\n",
    "        # 방금 down block에 나온 residual feature를 contronlet에서 나온 feature와 더해줌 => up block에 전달하기 위해\n",
    "        if is_controlnet:\n",
    "            new_down_block_res_samples = ()\n",
    "\n",
    "            for down_block_res_sample, down_block_additional_residual in zip(\n",
    "                down_block_res_samples, down_block_additional_residuals\n",
    "            ):\n",
    "                down_block_res_sample = down_block_res_sample + down_block_additional_residual\n",
    "                # 더한 뒤에 다시 튜플로 넣어줌\n",
    "                new_down_block_res_samples = new_down_block_res_samples + (down_block_res_sample,)\n",
    "\n",
    "            # upblock에 skip connection할 feature완성\n",
    "            down_block_res_samples = new_down_block_res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        if self.mid_block is not None:\n",
    "            sample = self.mid_block(sample, emb,\n",
    "                                    encoder_hidden_states=encoder_hidden_states,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                                    encoder_attention_mask=encoder_attention_mask,\n",
    "                                )\n",
    "\n",
    "        # controlnet에서 오는 feature가 잇으면 합쳐줌\n",
    "        if is_controlnet:\n",
    "            sample = sample + mid_block_additional_residual\n",
    "\n",
    "        # 5. up\n",
    "        for i, upsample_block in enumerate(self.up_blocks):\n",
    "            is_final_block = i == len(self.up_blocks) - 1\n",
    "\n",
    "            # Up block에 들어갈 skip connection 설정\n",
    "            # 먼저 제일 뒤에서부터 resdiual feature 뽑아냄\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            # 뽑아낸 제일 마지막은 제외해서 리스트를 다시 구성 (loop 돌면서 뒤에서 부터 하나씩 pop)\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            # final block이 아닌 경우는 upsample_block에서 upsample할 수 있게\n",
    "            if not is_final_block and forward_upsample_size:\n",
    "                upsample_size = down_block_res_samples[-1].shape[2:]\n",
    "\n",
    "                \n",
    "            # CA있으므로 이거 사용\n",
    "            if hasattr(upsample_block, \"has_cross_attention\") and upsample_block.has_cross_attention:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    res_hidden_states_tuple=res_samples,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    upsample_size=upsample_size,\n",
    "                    attention_mask=attention_mask,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                ...\n",
    "\n",
    "        # 6. post-process\n",
    "        if self.conv_norm_out:\n",
    "            sample = self.conv_norm_out(sample)\n",
    "            sample = self.conv_act(sample)\n",
    "        # 마지막 VAE 디코더로 들어가게끔 채널수 맞춰줌\n",
    "        sample = self.conv_out(sample)\n",
    "\n",
    "        ...\n",
    "        return UNet2DConditionOutput(sample=sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from diffusers import DDIMPipeline\n",
    "from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_openpose\", torch_dtype=torch.float16, cache_dir='/home/aiteam/tykim/hugging')\n",
    " \n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16,\n",
    "    cache_dir='/home/aiteam/tykim/hugging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
