{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def MLP(channels: list):\n",
    "    n = len(channels)\n",
    "    layers = []\n",
    "    for i in range(1, n):\n",
    "        layers.append(nn.Conv1d(channels[i-1], channels[i], kernel_size=1, bias=True))\n",
    "\n",
    "        if i < (n-1):\n",
    "            layers.append(nn.BatchNorm1d(channels[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, num_heads, feature_dim):\n",
    "        super().__init__()\n",
    "        assert feature_dim % num_heads == 0\n",
    "        self.dim = feature_dim // num_heads\n",
    "        self.num_heads = nun_heads\n",
    "        self.merge = nn.Conv1d(feature_dim, feature_dim, kernel_size=1)\n",
    "        self.proj = nn.ModuleList([deepcopy(self.merge) for _ in range(3)])\n",
    "\n",
    "    def forward(self, qeury, key, value):\n",
    "        # q : [B, feature_dim,  seq_len] : kernel =1 짜리 conv로 convolving\n",
    "        batch_dim = qeury.size(0)\n",
    "        # projection\n",
    "        query, key, value = [l(x).view(batch_dim, self.dim, self.num_heads, -1) for l, x in zip(self.proj, (query, key, value))]\n",
    "        x, prob = self.attention(query, key, value)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        dim = query.shape[1]\n",
    "        # batched matrix multiplication\n",
    "        scores = torch.einsum('bdhn,bdhm->bhnm', query, key) / dim ** .5\n",
    "        prob = F.softmax(scores, dim=-1)\n",
    "        return torch.einsum('bhnm,bdhm->bdhn', prob, value), prob\n",
    "\n",
    "class MessagePassing(nn.Module):\n",
    "    def __init__(self, feature_dim: int, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(num_heads, feature_dim)\n",
    "        self.mlp = MLP([feature_dim*2, feature_dim*2, out_dim])\n",
    "\n",
    "    def forward(self, features):\n",
    "        message, prob = self.attn(features, features, features)\n",
    "        return self.mlp(torch.cat([features, message], dim=1)), prob\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_heads=4):\n",
    "        super().__init__()\n",
    "\n",
    "        num_heads_in = num_heads\n",
    "        self.reshape = None\n",
    "        if input_dim != output_dim:\n",
    "            for num_heads_in in range(num_heads, 0, -1):\n",
    "                if input_dim % num_heads_in == 0:\n",
    "                    break\n",
    "            self.reshape = MLP([input_dim, output_dim])\n",
    "\n",
    "        self.attention = MessagePassing(input_dim, num_heads_in, out_dim=output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        message, prob = self.attnetion(features)\n",
    "        if self.reshape:\n",
    "            features = self.reshape(features)\n",
    "        output = features + message\n",
    "        return output, prob\n",
    "\n",
    "\n",
    "# torch geometric\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels, heads=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)  # Input: (num_nodes, num_node_features)\n",
    "        x = F.elu(self.conv1(x, edge_index))  # Output of conv1: (num_nodes, hidden_channels * heads)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)  # Output of conv2: (num_nodes, out_channels)\n",
    "        return F.log_softmax(x, dim=1)  # Final output: (num_nodes, out_channels)\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# import debugpy\n",
    "\n",
    "# debugpy.listen(('0.0.0.0', 5678))\n",
    "\n",
    "# print(\"Waiting for debugger attach\")\n",
    "# debugpy.wait_for_client()\n",
    "\n",
    "model = GAT(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    out_channels=dataset.num_classes, \n",
    "    hidden_channels=8, \n",
    "    heads=8\n",
    ").to('cuda')\n",
    "data = data.to('cuda')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, data.edge_index), []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        pred = logits[mask].max(1)[1] # max 함수를 dim=1에 대해 수행 후 값[0]과 인덱스[1]를 얻음\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, Val: {accs[1]:.4f}, Test: {accs[2]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
