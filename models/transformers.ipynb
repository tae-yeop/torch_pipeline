{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae-yeop/transformer-adventure/blob/main/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKI7pY9ZhDyz"
      },
      "source": [
        "https://github.com/The-AI-Summer/self-attention-cv/blob/8280009366b633921342db6cab08da17b46fdf1c/self_attention_cv/transformer_vanilla/transformer_block.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb6Z8m4QUsiy",
        "outputId": "1c0664a2-fb8b-4714-e796-1b35249e28f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2iKHDs0TwUI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from einops import rearrange\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Einsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.rand((2,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Permutation of Tensors\n",
        "torch.einsum(\"ij->ji\", x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summation\n",
        "torch.einsum(\"ij->\",x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column sum\n",
        "torch.einsum(\"ij->j\", x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Row sum\n",
        "torch.einsum(\"ij->i\", x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mat-vector mul\n",
        "v = torch.rand((1,3))\n",
        "torch.einsum(\"ik, jk-> ij\", x, v)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mat- MAt mul\n",
        "x.mm(x.t())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.einsum('ij,kj->ik', x,x) #2x2 = 2x3 x 3x2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dot product first row with first row of matrix\n",
        "torch.einsum(\"i,i->\", x[0], x[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dot product with matrix\n",
        "torch.einsum(\"ij,ij->\", x,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hadarmard product (element-wise mul)\n",
        "torch.einsum(\"ij,ij->ij\",x,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outer Product\n",
        "a = torch.rand((3))\n",
        "b = torch.rand((5))\n",
        "torch.einsum(\"i,j->ij\", a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch Mat Mul torch.bmm\n",
        "a = torch.rand((3,2,5))\n",
        "b = torch.rand((3,5,3))\n",
        "torch.einsum(\"ijk, ikl->ijl\", a,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# matrix diagonal\n",
        "x = torch.rand((3,3))\n",
        "print(x)\n",
        "print(torch.einsum(\"ii->i\",x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# matrix trace\n",
        "torch.einsum(\"ii->\",x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ZQ9SMWbchi"
      },
      "source": [
        "# Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A9hYT2QUhX5"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.to_qvk = nn.Linear(dim, dim*3, bias=False)\n",
        "    self.scale_factor = dim ** -0.5\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    \"\"\"\n",
        "    x : [b, T, d]\n",
        "    Returns : [b, T, d]\n",
        "    \"\"\"\n",
        "    assert x.dim() == 3, '3D tensor must be provided'\n",
        "    # [B, tokens, dim*3]\n",
        "    qvk = self.to_qvk(x)\n",
        "\n",
        "    q, k, v = tuple(rearrange(qvk, 'b t (d k) -> k b t d', k=3))\n",
        "\n",
        "    # [batch, tokens, tokens]\n",
        "    scaled_dot_prod = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale_factor\n",
        "\n",
        "    if mask is not None:\n",
        "      # check shape = [tokens, tokens]\n",
        "      assert mask.shape == scaled_dot_prod.shape[1:]\n",
        "      scaled_dot_prod = scaled_dot_prod.masked_fill(mask==0, -np.inf)\n",
        "    attention = torch.softmax(scaled_dot_prod, dim=-1)\n",
        "    return torch.einsum('b i j, b j d -> b i d', attention, v)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bwSpkflbIJW",
        "outputId": "36ffadb4-45cf-4166-8755-b727be583660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 16, 32])"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test\n",
        "sa = SelfAttention(32)\n",
        "t = torch.randn((8, 16, 32))\n",
        "sa(t).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXvwqHQNa9cu"
      },
      "source": [
        "# MHSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFtluLZ7bhRX"
      },
      "outputs": [],
      "source": [
        "def compute_mhsa(q, k, v, scaled_factor=1, mask=None):\n",
        "  \"\"\"\n",
        "  Returns : [batch, heads, tokens, dim]\n",
        "  \"\"\"\n",
        "  # scaled_dot_prod.shape = [b, h, token, token]\n",
        "  scaled_dot_prod = torch.einsum('... i d, ... j d -> ... i j', q, k) * scaled_factor\n",
        "\n",
        "  if mask is not None:\n",
        "    assert mask.shape == scaled_dot_prod.shape[2:]\n",
        "    scaled_dot_prod = scaled_dot_prod.masked_fill(maks==0, -np.inf)\n",
        "\n",
        "  attention = torch.softmax(scaled_dot_prod, dim=-1)\n",
        "  return torch.einsum('... i j, ... j d -> ... i d', attention, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgfTeexocwOJ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, dim, heads=8, dim_head=None):\n",
        "    \"\"\"\n",
        "    Implementation of multi-head attention layer of the original transformer model.\n",
        "    einsum and einops.rearrange is used whenever possible\n",
        "    Args:\n",
        "        dim: token's dimension, i.e. word embedding vector size\n",
        "        heads: the number of distinct representations to learn\n",
        "        dim_head: the dim of the head. In general dim_head<dim.\n",
        "        However, it may not necessary be (dim/heads)\n",
        "    \"\"\"\n",
        "    # dim이 head의 배수로 맞아떨어지지 않는 경우까지 고려했음\n",
        "    super().__init__()\n",
        "    self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
        "    _dim = self.dim_head * heads\n",
        "\n",
        "    self.heads = heads\n",
        "    self.to_qvk = nn.Linear(dim, _dim*3, bias=False)\n",
        "    \n",
        "    self.W_0 = nn.Linear(_dim, dim, bias=False)\n",
        "    self.scale_factor = self.dim_head * -0.5\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    assert x.dim() == 3\n",
        "    qkv = self.to_qvk(x)\n",
        "\n",
        "    q,k,v = tuple(rearrange(qkv, 'b t (d k h) -> k b h t d', k=3, h=self.heads))\n",
        "\n",
        "    out = compute_mhsa(q, k, v, self.scale_factor)\n",
        "\n",
        "    out = rearrange(out, 'b h t d -> b t (h d)')\n",
        "    return self.W_0(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u7Dc3dpfVH-",
        "outputId": "e71e1788-6906-46c5-fba1-ba33ab644aea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 16, 32])"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mhsa = MultiHeadSelfAttention(32, 8)\n",
        "t = torch.randn((8, 16, 32))\n",
        "mhsa(t).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHbBhvNlfdVP"
      },
      "source": [
        "# Vanilla Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T71ubEEnwm2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from einops import repeat\n",
        "from torch import Tensor, nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edVeoi9Wf4nS"
      },
      "outputs": [],
      "source": [
        "def expand_to_batch(tensor, desire_size):\n",
        "  tile = desired_size // tensor.shape[0]\n",
        "  return repeat(tensor, 'b ... -> (b tile) ...', tile=tile)\n",
        "\n",
        "def init_random_seed(seed, gpu=False):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  os.envision['PYTHONHASHSEED'] = str(seed)\n",
        "  if gpu:\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# from https://huggingface.co/transformers/_modules/transformers/modeling_utils.html\n",
        "def get_module_device(parameter : nn.Module):\n",
        "  try:\n",
        "    return next(parameter.parameters()).device\n",
        "  except StopIteration:\n",
        "    # For nn.DataParallel compatibility in PyTorch 1.5\n",
        "    def find_tensor_attributes(module : nn.Module) -> List[Tuple[str, Tensor]]:\n",
        "      tuples = [(k, v) for k,v in module.__dict__.items() if torch.is_tensor(v)]\n",
        "      return tuples\n",
        "  gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n",
        "  first_tuple = next(gen)\n",
        "  return first_tuple[1].device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifbZIOSqrJ94"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, dim, heads=8, dim_head=None, dim_linear_block=1024, dropout=0.1, activation=nn.GELU,\n",
        "               mhsa=None, prenorm=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dim: token's vector length\n",
        "        heads: number of heads\n",
        "        dim_head: if none dim/heads is used\n",
        "        dim_linear_block: the inner projection dim\n",
        "        dropout: probability of droppping values\n",
        "        mhsa: if provided you can change the vanilla self-attention block\n",
        "        prenorm: if the layer norm will be applied before the mhsa or after\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.mhsa = mhsa if mhsa is not None else MultiHeadSelfAttention(dim=dim, heads=heads, dim_head_dim = dim_head)\n",
        "    self.prenorm = prenorm\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "    self.norm_1 = nn.LayerNorm(dim)\n",
        "    self.norm_2 = nn.LayerNorm(dim)\n",
        "\n",
        "    self.linear = nn.Sequential(nn.Linear(dim, dim_linear_block), \n",
        "                                activation(), \n",
        "                                nn.Dropout(dropout),\n",
        "                                nn.Linear(dim_linear_block, dim),\n",
        "                                nn.Dropout(dropout))\n",
        "  \n",
        "  def forward(self, x, mask=None):\n",
        "    if self.prenorm:\n",
        "      y = self.drop(self.mhsa(self.norm_1(x), mask)) + x\n",
        "      out = self.linear(self.norm_2(y)) + y\n",
        "    else:\n",
        "      y = self.norm_1(self.drop(self.mhsa(x, mask)) + x)\n",
        "      out = self.norm_2(self.linear(y) + y)\n",
        "\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z2ddJ5dtNXI"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, dim, blocks=6, heads=8, dim_head=None, dim_linear_block=1024, dropout=0, prenorm=False):\n",
        "    super().__init__()\n",
        "    self.block_list = [TransformerBlock(dim, heads, dim_head, dim_linear_block, dropout, prenorm=prenomr) for _ in range(blocks)]\n",
        "    self.layers = nn.ModuleList(self.block_list)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iCUG4D5md2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFxZBv3u5m9G"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def get_clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jMrrCvV57ep"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "    super().__init__()\n",
        "    self.N = N\n",
        "    self.embed = Embedder(vocab_size, d_model)\n",
        "    self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "    self.layers = get_clones(Encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ViT\n",
        "- https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py\n",
        "- https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n",
        "- https://github.dev/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from typing import Callable, List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from itertools import repeat\n",
        "import collections.abc\n",
        "\n",
        "\n",
        "# From PyTorch internals\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n",
        "            return tuple(x)\n",
        "        return tuple(repeat(x, n))\n",
        "    return parse\n",
        "\n",
        "\n",
        "to_1tuple = _ntuple(1)\n",
        "to_2tuple = _ntuple(2)\n",
        "to_3tuple = _ntuple(3)\n",
        "to_4tuple = _ntuple(4)\n",
        "to_ntuple = _ntuple\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" 2D Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    dynamic_img_pad: torch.jit.Final[bool]\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            img_size: Optional[int] = 224,\n",
        "            patch_size: int = 16,\n",
        "            in_chans: int = 3,\n",
        "            embed_dim: int = 768,\n",
        "            norm_layer: Optional[Callable] = None,\n",
        "            flatten: bool = True,\n",
        "            bias: bool = True,\n",
        "            strict_img_size: bool = True,\n",
        "            dynamic_img_pad: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.patch_size = to_2tuple(patch_size)\n",
        "        if img_size is not None:\n",
        "            self.img_size = to_2tuple(img_size)\n",
        "            self.grid_size = tuple([s // p for s, p in zip(self.img_size, self.patch_size)])\n",
        "            self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "        else:\n",
        "            self.img_size = None\n",
        "            self.grid_size = None\n",
        "            self.num_patches = None\n",
        "\n",
        "        self.strict_img_size = strict_img_size\n",
        "        self.dynamic_img_pad = False\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "        self.flatten = False\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        if self.dynamic_img_pad:\n",
        "            pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]\n",
        "            pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]\n",
        "            x = F.pad(x, (0, pad_w, 0, pad_h))\n",
        "        x = self.proj(x)\n",
        "        if self.flatten:\n",
        "            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n",
        "        x = self.norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ViTPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.mean_pool = config.mean_pool\n",
        "        \n",
        "    def forward(self, hidden_states):\n",
        "        # first token or mean token\n",
        "        hidden_states = hidden_states.mean(dim=1) if self.mean_pool else hidden_states[:, 0]\n",
        "        pooled_output = self.dense(hidden_states)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_hidden_layers = config.num_hidden_layers\n",
        "        self.norm = nn.LayerNorm(self.hidden_size)\n",
        "        self.layers = nn.ModuleList([ViTLayer(config) for _ in range(self.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, output_hidden_states: bool = False):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        \n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "            layer_outputs = layer_module(hidden_states)\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states, )\n",
        "\n",
        "\n",
        "        return ViTEncoderOutput(last_hidden_state=hidden_states,\n",
        "                                hidden_states=all_hidden_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ViTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        image_height, image_width = config.image_size, config.image_size\n",
        "        self.patch_size = config.patch_size\n",
        "        self.num_channels = config.num_channels\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.drop_rate = config.drop_rate\n",
        "\n",
        "        \n",
        "        assert image_height % self.patch_size == 0 and image_width % self.patch_size == 0, 'Image dimensions must be divisible by the patch size'\n",
        "        num_patches = (image_height // self.patch_size) * (image_width // self.patch_size)\n",
        "        patch_dim = self.num_channels * self.patch_size * self.patch_size\n",
        "\n",
        "        self.patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size),\n",
        "            nn.LayerNorm(patch_dim), # pre-norm\n",
        "            nn.Linear(patch_dim, self.hidden_size),\n",
        "            nn.LayerNorm(self.hidden_size)\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches +1, self.hidden_size))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.hidden_size))\n",
        "        self.dropout = nn.Dropout(self.drop_rate)\n",
        "\n",
        "        self.encoder = ViTEncoder(config)\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.pooler = ViTPooler(config) if config.add_pooling_layer else None\n",
        "\n",
        "    def forwrad(self, pixel_values):\n",
        "        embedding_output = self.patch_embedding(pixel_values)\n",
        "        batch, seq_len,_ = embedding_output.shape\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=batch)\n",
        "        embedding_output = torch.cat((cls_tokens, embedding_output), dim=1)\n",
        "        # x가 더 작은 크기가 오면 여기서 n\n",
        "        embedding_output += self.pos_embedding[:, :(seq_len+1)]\n",
        "        embedding_output = self.dropout(embedding_output)\n",
        "\n",
        "        sequence_output = self.encoder(embedding_output)\n",
        "        sequence_output = self.layernorm(sequence_output)\n",
        "\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        return sequence_output, pooled_output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNUuvXevwut60kbvzcUchPt",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
