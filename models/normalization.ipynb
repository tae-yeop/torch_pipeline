{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer('beta', torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 함수 형태로\n",
    "def Normalize(in_channels, norm_type='group', num_groups=32):\n",
    "    if norm_type == 'batchnorm':\n",
    "        return torch.nn.BatchNorm2d(num_features=in_channels)\n",
    "    else:\n",
    "        return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# affine transform 없는 버전(학습 안하는 버전)\n",
    "from typing import Union, List\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "batch_size, seq_size, embed_dim = 2, 3, 4\n",
    "embedding = torch.randn(batch_size, seq_size, embed_dim)\n",
    "print(\"x: \", embedding)\n",
    "print(embedding.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "layer_norm = torch.nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
    "layer_norm_output = layer_norm(embedding)\n",
    "print(\"y: \", layer_norm_output)\n",
    "print(layer_norm_output.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "def custom_layer_norm(\n",
    "        x: torch.Tensor, dim: Union[int, List[int]] = -1, eps: float = 0.00001\n",
    ") -> torch.Tensor:\n",
    "    mean = torch.mean(x, dim=(dim,), keepdim=True)\n",
    "    var = torch.square(x - mean).mean(dim=(dim,), keepdim=True)\n",
    "    return (x - mean) / torch.sqrt(var + eps)\n",
    "\n",
    "\n",
    "custom_layer_norm_output = custom_layer_norm(embedding)\n",
    "print(\"y_custom: \", custom_layer_norm_output)\n",
    "print(custom_layer_norm_output.shape)\n",
    "\n",
    "assert torch.allclose(layer_norm_output, custom_layer_norm_output), 'Tensors do not match.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    LayerNorm 초기화 편하게 하기\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def calc_activation_shape(\n",
    "        dim, ksize, dilation=(1, 1), stride=(1, 1), padding=(0, 0)\n",
    "    ):\n",
    "        def shape_each_dim(i):\n",
    "            odim_i = dim[i] + 2 * padding[i] - dilation[i] * (ksize[i] - 1) - 1\n",
    "            return (odim_i / stride[i]) + 1\n",
    "\n",
    "\n",
    "        return shape_each_dim(0), shape_each_dim(1)\n",
    "\n",
    "\n",
    "    def __init__(self, idim, num_classes=10):\n",
    "        self.layer1 = torch.nn.Conv2D(3, 5, 3)\n",
    "        ln_shape = Network.calc_activation_shape(idim, 3) # <--- Calculate the shape of output of Convolution\n",
    "        self.norm1 = torch.nn.LayerNorm([5, *ln_shape]) # <--- Normalize activations over C, H, and W (see fig.above)\n",
    "        self.layer2 = torch.nn.Conv2D(5, 10, 3)\n",
    "        ln_shape = Network.calc_activation_shape(ln_shape, 3)\n",
    "        self.norm2 = torch.nn.LayerNorm([10, *ln_shape])\n",
    "        self.layer3 = torch.nn.Dense(num_classes)\n",
    "\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = F.relu(self.norm1(self.layer1(input)))\n",
    "        x = F.relu(self.norm2(self.layer2(x)))\n",
    "        x = F.sigmoid(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 파이토치 텐서 생성\n",
    "data = torch.randn(100, 10)  # 100개의 샘플과 10개의 피처\n",
    "\n",
    "# 정규화 수행\n",
    "mean = data.mean(dim=0)\n",
    "std = data.std(dim=0)\n",
    "data_normalized = (data - mean) / std\n",
    "\n",
    "# 텐서를 NumPy 배열로 변환 (시각화를 위해)\n",
    "data_np = data.numpy()\n",
    "data_normalized_np = data_normalized.numpy()\n",
    "\n",
    "# 기술통계 출력\n",
    "print(\"Mean before normalization:\", mean)\n",
    "print(\"Standard deviation before normalization:\", std)\n",
    "print(\"Mean after normalization:\", data_normalized.mean(dim=0))\n",
    "print(\"Standard deviation after normalization:\", data_normalized.std(dim=0))\n",
    "\n",
    "# 첫 번째 피처에 대한 히스토그램 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data_np[:, 0], kde=True, color='blue')\n",
    "plt.title('Histogram before Normalization')\n",
    "plt.xlabel('Feature Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data_normalized_np[:, 0], kde=True, color='red')\n",
    "plt.title('Histogram after Normalization')\n",
    "plt.xlabel('Feature Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "plt.title('Sample Plot')\n",
    "plt.savefig('/workspace/0_practice/torch/features/image.png')  # 이미지 파일로 저장\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 데이터 생성\n",
    "data = torch.randn(4, 3, 32, 32)  # 정규 분포에서 무작위 데이터 생성\n",
    "\n",
    "# 정규화 층 초기화\n",
    "batch_norm = nn.BatchNorm2d(3)  # 채널 수에 맞춰서\n",
    "layer_norm = nn.LayerNorm([3, 32, 32])  # 정규화할 차원을 명시\n",
    "group_norm = nn.GroupNorm(3, 3)  # 그룹 수와 채널 수\n",
    "\n",
    "# 정규화 적용\n",
    "data_bn = batch_norm(data.clone())  # 데이터를 복제하여 정규화\n",
    "data_ln = layer_norm(data.clone())\n",
    "data_gn = group_norm(data.clone())\n",
    "\n",
    "# 시각화 함수\n",
    "def plot_histogram(data, title):\n",
    "    data = data.numpy().flatten()  # 히스토그램을 위해 데이터를 1차원 배열로 변환\n",
    "    sns.histplot(data, kde=True, bins=30, color='blue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# 원본 데이터 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_histogram(data, 'Original Data Histogram')\n",
    "\n",
    "# 배치 정규화 데이터 시각화\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_histogram(data_bn, 'BatchNorm Data Histogram')\n",
    "\n",
    "# 레이어 정규화 데이터 시각화\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_histogram(data_ln, 'LayerNorm Data Histogram')\n",
    "\n",
    "# 그룹 정규화 데이터 시각화\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_histogram(data_gn, 'GroupNorm Data Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class InstanceNorm(Module):\n",
    "    \"\"\"\n",
    "    https://nn.labml.ai/normalization/instance_norm/index.html\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, eps=1e-5, affine=True):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.scale = nn.Parameter(torch.ones(channels))\n",
    "        self.shift = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "    def forward(slef, x:torch.Tensor):\n",
    "        x_shape = x.shape\n",
    "        batch_size = x_shape[0]\n",
    "\n",
    "        assert self.channels = x.shape[1]\n",
    "        x = x.view(batch_size, self.channels, -1)\n",
    "\n",
    "        mean = x.mean(dim=-1, keepdim=True) # E[x]\n",
    "\n",
    "        mean_x2 = (x**2).mean(dim=-1, keepdim=True) # E[x^2]\n",
    "\n",
    "        var = mean_x2 - mean**2 # E[x^2] - E[x]^2\n",
    "\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        if self.affine:\n",
    "        x_norm = self.scale.view(1, -1, 1) * x_norm + self.shift.view(1, -1, 1)\n",
    "\n",
    "        return x_norm.view(x_shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
