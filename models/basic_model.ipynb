{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_method_dict = {\n",
    "    'zero': nn.init.zeros_,\n",
    "    'constant': lambda w: nn.init.constant_(w, 0.01),\n",
    "    'uniform': nn.init.uniform_,\n",
    "    'kaiming-he': nn.init.kaiming_normal_,\n",
    "    'xavier-glorot': nn.init.xavier_uniform_,\n",
    "}\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # model_args.in_dim -> Flatten 후의 입력 차원이라고 가정\n",
    "        # model_args.out_dim -> 최종 출력 차원 (예: 분류 클래스 수 등)\n",
    "        self.linear = nn.Linear(model_args.in_dim, model_args.out_dim, bias=model_args.bias)\n",
    "\n",
    "        # 초깃값 세팅\n",
    "        self.reset_parameters(model_args.init_method)\n",
    "\n",
    "\n",
    "    def reset_parameters(self, init_method):\n",
    "        if init_method not in init_method_dict:\n",
    "            raise ValueError(f\"Unknown init_method: {init_method}\")\n",
    "        init_fn = init_method_dict[init_method]\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            # 자기 자신(Model)도 named_modules에 등장하므로, 분기 필요\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                # weight 초기화\n",
    "                init_fn(module.weight)\n",
    "                # bias 초기화\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # (B, C, H, W)를 (B, -1)로 펼쳐서 Linear에 투입\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력 클래스\n",
    "- 단순 dict 반환보다 유연한 설계, 가독성\n",
    "- IDE 수준에서 오타 발견"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ModelOutput(OrderedDict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            key = list(self.__dict__.keys())[key]\n",
    "        return getattr(self, key, None)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        setattr(self, key, value)\n",
    "        super().__setitem__(key, value)\n",
    "\n",
    "\n",
    "@dataclass(init=False)\n",
    "class EncoderOutput(ModelOutput):\n",
    "    last_hidden_state = None\n",
    "    hidden_states = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = EncoderOutput(\n",
    "            last_hidden_state=1,\n",
    "            hidden_states=2)\n",
    "t.last_hidden_state, t.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            embed_dim=16, num_layers=2, return_intermediate=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.return_intermediate = return_intermediate\n",
    "        \n",
    "        # 임베딩 레이어 (예시: Flatten + Linear 등)\n",
    "        self.to_patch_embed = nn.Sequential(\n",
    "            nn.Conv2d(3, embed_dim, kernel_size=4, stride=4),  # (B,3,H,W)->(B,embed_dim,H/4,W/4)\n",
    "            nn.Flatten(start_dim=2),                           # (B, embed_dim, (H/4)*(W/4))\n",
    "            Rearrange('b c s -> b s c')                                 # (B, seq, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Transformer 레이어들을 매우 간단히 흉내\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(embed_dim, embed_dim), nn.ReLU())\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1) Patch Embedding\n",
    "        out = self.to_patch_embed(x)   # shape: (B, seq_len, embed_dim)\n",
    "\n",
    "        hidden_states = []\n",
    "        current = out\n",
    "        # 2) Encoder layers 순회\n",
    "        for layer in self.layers:\n",
    "            current = layer(current)   # shape 유지: (B, seq_len, embed_dim)\n",
    "            if self.return_intermediate:\n",
    "                hidden_states.append(current)\n",
    "\n",
    "        # 3) 마지막 hidden state\n",
    "        last_hidden_state = current   # (B, seq_len, embed_dim)\n",
    "\n",
    "        # 4) Output\n",
    "        # ModelOutput 형태로 반환\n",
    "        return EncoderOutput(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            hidden_states=hidden_states if self.return_intermediate else None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 32, 32)\n",
    "model = Encoder(embed_dim=16, num_layers=2, return_intermediate=True)\n",
    "output = model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state shape: torch.Size([2, 64, 16])\n",
      "hidden_states[0] shape: torch.Size([2, 64, 16])\n",
      "hidden_states[1] shape: torch.Size([2, 64, 16])\n",
      "Check by dict key: torch.Size([2, 64, 16])\n",
      "Check by index [0]: torch.Size([2, 64, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"last_hidden_state shape:\", output.last_hidden_state.shape)\n",
    "if output.hidden_states is not None:\n",
    "    for i, hs in enumerate(output.hidden_states):\n",
    "        print(f\"hidden_states[{i}] shape:\", hs.shape)\n",
    "\n",
    "# 딕셔너리 처럼도 접근 가능\n",
    "print(\"Check by dict key:\", output[\"last_hidden_state\"].shape)\n",
    "# 인덱스로도 가능 (0 -> 'last_hidden_state', 1-> 'hidden_states')\n",
    "print(\"Check by index [0]:\", output[0].shape if output[0] is not None else None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
