{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562e728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e57d27",
   "metadata": {},
   "source": [
    "### 메타데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741b9a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "\n",
    "\n",
    "builder = load_dataset_builder(\"huggan/pokemon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e91164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.packaged_modules.parquet.parquet.ParquetPokemon'>\n"
     ]
    }
   ],
   "source": [
    "print(type(builder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "800ad950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='',\n",
      "            citation='',\n",
      "            homepage='',\n",
      "            license='',\n",
      "            features={'image': Image(mode=None, decode=True, id=None)},\n",
      "            post_processed=None,\n",
      "            supervised_keys=None,\n",
      "            builder_name='parquet',\n",
      "            dataset_name='pokemon',\n",
      "            config_name='default',\n",
      "            version=0.0.0,\n",
      "            splits={'train': SplitInfo(name='train',\n",
      "                                       num_bytes=141969739,\n",
      "                                       num_examples=7357,\n",
      "                                       shard_lengths=None,\n",
      "                                       dataset_name=None)},\n",
      "            download_checksums=None,\n",
      "            download_size=130657589,\n",
      "            post_processing_size=None,\n",
      "            dataset_size=141969739,\n",
      "            size_in_bytes=192036232.655)\n"
     ]
    }
   ],
   "source": [
    "pprint(builder.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95cf6b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features : {'image': Image(mode=None, decode=True, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"features :\", builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea04c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits   : {'train': SplitInfo(name='train', num_bytes=141969739, num_examples=7357, shard_lengths=None, dataset_name=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"splits   :\", builder.info.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a240a81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total MB : 135.39\n"
     ]
    }
   ],
   "source": [
    "print(\"total MB :\", round(builder.info.dataset_size / 1024**2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40ba799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f9fa522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100.0000,  88.8889,  77.7778,  66.6667,  55.5556,  44.4444,  33.3333,\n",
       "         22.2222,  11.1111,   0.0000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(100, 0, 10, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f50f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25597bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = 'Convert the equation images to LaTeX equations.'\n",
    "def convert(sample):\n",
    "    conversation=[\n",
    "        {\n",
    "            'role' : 'user',\n",
    "            'content' : [\n",
    "                {'type': 'text', 'text': instruction},\n",
    "                {'type': 'image', 'image': sample['image']}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'role' : 'assistant',\n",
    "            'content' : [\n",
    "                {'type': 'text', 'text': sample['text']}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {'message': conversation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a11f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vision_info(messages):\n",
    "    image_inputs = []\n",
    "    for msg in messages:\n",
    "        content = mes.get(\"content\", [])\n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "        \n",
    "        for element in content:\n",
    "            if isinstance(element, dict) and (\"image\" in element or element.get(\"type\") == \"image\"):\n",
    "                image = element[\"image\"]\n",
    "                image_inputs.append(image.convert(\"RGB\"))\n",
    "\n",
    "    return image_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8241663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|████████████████████████████████████████| 68686/68686 [00:01<00:00, 52232.71 examples/s]\n",
      "Generating test split: 100%|███████████████████████████████████████████| 7632/7632 [00:00<00:00, 58054.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\t\n",
    "# Load dataset from the hub\n",
    "dataset_train = load_dataset('unsloth/LaTeX_OCR', split='train[:3000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec32a0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/3000 [00:26<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 3000/3000 [00:01<00:00, 2281.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Convert the equation images to LaTeX equations.'}, {'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x40 at 0x764CEA7770D0>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '{ \\\\frac { N } { M } } \\\\in { \\\\bf Z } , { \\\\frac { M } { P } } \\\\in { \\\\bf Z } , { \\\\frac { P } { Q } } \\\\in { \\\\bf Z }'}]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "converted_dataset_train = [\n",
    "    convert(sample) \\\n",
    "    for sample in tqdm(dataset_train, total=len(dataset_train))\n",
    "]\n",
    "print(converted_dataset_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f9776a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoModelForImageTextToText' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoModelForImageTextToText' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21993868",
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "        images.append(image_inputs)\n",
    " \n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    " \n",
    "    # The labels are the input_ids, and we mask the padding tokens and image tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    " \n",
    "    # Mask image tokens\n",
    "    image_token_id = [\n",
    "        processor.tokenizer.convert_tokens_to_ids(\n",
    "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "        )\n",
    "    ]\n",
    "    # Mask tokens for not being used in the loss computation\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    " \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
